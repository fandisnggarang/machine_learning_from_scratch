# Machine Learning Algorithms from Scratch
Sharing my learning experience!

The final phase of my learning journey was Deep Learning. I spent some time studying Neural Networks and similar concepts but decided to take a step back. I revisited more traditional machine learning algorithms such as: 
Linear/Logistic Regression, KNN Classifier/Regressor, SVC/SVR, Decision Tree Classifier/Regressor, Random Forest Classifier/Regressor, AdaBoost Classifier/Regressor, and KMeans Clustering.

The focus was on building these algorithms (basic versions) from scratch: understanding their logic and mathematical foundations. I explored multiple resources, studied their code, and experimented with modifications. Most of the resources focused on classification algorithms, so I extended their functionality to include regression variants.

The easiest algorithms to grasp were Linear and Logistic Regression. A valuable insight from this process was the importance of initializing weights and biases with appropriate starting values to avoid NaN errors during optimization—a common issue when developing neural networks.

The most challenging was understanding Decision Tree Classifiers and Regressors, particularly the recursive logic for tree construction: how a method calls itself and halts when the stopping criteria are met.  I spent a lot of time imagining the process of how the tree is formed step by step, which helped me grasp its intricacies better.

I also spent considerable time understanding how the hyperplane in SVM adjusts for effective classification. While I didn’t implement kernel methods for handling non-linear data, I explored their concepts and how they expand the SVM's capabilities. 

If you find any parts that need improvement, please let me know. I’d greatly appreciate it!
